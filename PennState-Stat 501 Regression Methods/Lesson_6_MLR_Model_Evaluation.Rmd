---
title: "PennState Stat 501 Lesson 6 - MLR Model Evaluation"
output: 
   html_document:
    toc: yes
    toc_depth: 3
    css: ../style.css
params:
  date: !r Sys.Date()    
---

```{r,setup, include=FALSE, eval=TRUE}
options(knitr.table.format = "html", width = 140)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 12, fig.height = 8)
```

<div>Author: Rick Dean</div>
<div>Article date: `r params$date`</div>

<div class="abstract">
  <p class="abstract">Abstract</p>
The following notes, and R scripts are based on the online course [PennState Lesson 6 MLR Model Evaluation](https://online.stat.psu.edu/stat501/lesson/6)  
</div>  

```{r, warning=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
library(data.table)
library(RregressPkg)
library(RplotterPkg)
library(ggplot2)
library(here)

current_dir <- here::here("PennState-Stat 501 Regression Methods")
```

## 6.1 Three types of hypotheses

Research questions:

a. Is a regression model containing at least one predictor useful in predicting the response:
$H_{0}: \beta_{1} = \beta_{2} = \beta_{3}...\beta_{k} = 0$
$H_{A}:$ At least on $\beta_{i} \ne 0$ (for $i$ = 1,2,3...k)

b. Is a specific predictor $\beta_{i}$ not equal to zero:
$H_{0}: \beta_{i} = 0$
$H_{A}: \beta_{i} \ne 0$

c. Is a subset(more than one but not all) of the predictors simultaneously zero:
$H_{0}: \beta_{p} = \beta_{p+1} = \beta_{p+2} = 0
$H_{A}:$ At least one $\beta_{i} \ne 0$ (for i = p,p+1,p+2)

### Example 6-1: Heart attacks in rabbits
Primary research question:
<blockquote>Does the mean size of the infarcted area differ among the three treatment groups -- no cooling, early cooling, and late cooling -- when controlling for the size of the region at risk for infarction</blockquote>

The regression model:

$$area = \beta_{intercept} + \beta_{nocool} + \beta_{early}  + \beta_{late} + \epsilon$$

1. Read the data:
```{r}
data_path <- file.path(current_dir, "data/coolhearts.txt")
coolhearts_dt <- data.table::fread(data_path)
coolhearts_dt <- coolhearts_dt[, .(Infect = `Inf`, Area = Area, Early = X2, Late = X3, Group = Group)]
```

2. Estimate the OLS
```{r}
coolhearts_ols_lst <- RregressPkg::ols_calc(
  df = coolhearts_dt,
  formula_obj = Infect ~ Area + Early + Late
)
```

3. Show the coefficients:
```{r}
RplotterPkg::create_table(x = coolhearts_ols_lst$coef_df, caption = "Infect ~ Area + Early + Late")
```

4. Prepare data frame for plotting:
```{r}
coolhearts_dt[, Group := fifelse(Group == 3,"Control",
                    fifelse(Group == 1,"Early",
                    fifelse(Group == 2,"Late"," ")))]
coolhearts_dt[, Fit := coolhearts_ols_lst$fitted_val]
```

5. Plot the three conditions:
```{r}
RplotterPkg::create_scatter_plot(
  df = coolhearts_dt,
  aes_x = "Area",
  aes_y = "Infect",
  aes_fill = "Group",
  title = "Infarcted vs Area",
  subtitle = "Grouping by cooling early/late/control of rabbits",
  rot_y_tic_label = T,
  pts_size = 2,
  x_title = "Size of Area at Risk(grams)",
  y_title = "Size of Infarcted Area(grams)"
) + geom_line(aes(x = Area, y = Fit, color = Group))

```

## 6.2 General linear F-test

The "general linear F-test" involves three basic steps:

a. Define a larger **full model** or **unrestricted model** (one with more parameters)

b. Define a smaller **reduced model** or **restricted model** (one with fewer parameters)

c. Use the **F-statistic** to decide whether or not to reject the smaller reduced model in favor of the larger full model


The $F$-statistic is defined as follows:

$$ F = \left ( \frac{SSE_{r} - SSE_{ur}}{df_{r} - df_{ur}} \right ) \div \left ( \frac{SSE_{ur}}{df_{ur}} \right ) $$
Note q = $df_{r} - df_{ur}$ = the difference in the number of predictors between the full and reduced models

An alternative definition from $F$ involves the $R^2$ values from the full and reduced models -- known as the **R-squared form of the F statistic**:
$$F = \frac{(R_{ur}^2 - R_{r}^2)/q}{(1 - R_{ur}^2)/df_{ur}}$$

We use the general linear $F$-statistic to decide whether or not:

a. to reject the null hypothesis $H_{0}$: the reduced model

b. in favor of the alternative hypothesis $H_{A}$: the full model

### Example 6-2: Alcohol and muscle strength data

The Hypotheses' --

Reduced model:
$$H_{0}: y_{strength} = \beta_{intercept} + \epsilon$$

Full model:
$$H_{A}: y_{strength} = \beta_{intercept} + \beta_{alcohol} + \epsilon$$

1. Set the data:
```{r}
data_path <- file.path(current_dir, "data/alcoholarm.txt")
alchoholarm_dt <- data.table::fread(data_path)
```

2. Estimate the reduced and full models:
```{r}
# reduced_ols <- mean(alchoholarm_dt$strength)
# reduced_sse <- sum((alchoholarm_dt$strength - reduced_ols)^2)
# using only the intercept in the model
reduced_ols <- RregressPkg::ols_calc(
  df = alchoholarm_dt,
  formula_obj = strength ~ 1
)
reduced_sse <- reduced_ols$sse

full_ols <- RregressPkg::ols_calc(
  df = alchoholarm_dt,
  formula_obj = strength ~ alcohol
)
full_sse <- full_ols$sse
```

3. Create the plot data frame and display the reduced and full models:
```{r, fig.width=13, fig.height=8}
plot_df <- data.frame(
  Alcohol = alchoholarm_dt$alcohol,
  Strength = alchoholarm_dt$strength,
  Strength_R = reduced_ols$fitted_val,
  Strength_F = full_ols$fitted_val
)

reduced_plot <- RplotterPkg::create_scatter_plot(
  df = plot_df,
  aes_x = "Alcohol",
  aes_y = "Strength",
  title = "Reduced Model",
  subtitle = paste0("strength ~ mean(strength); SSE = ",round(reduced_sse,digits = 2)),
  rot_y_tic_label = T
) + geom_line(aes(y = Strength_R), color="red")
#reduced_plot

full_plot <- RplotterPkg::create_scatter_plot(
  df = plot_df,
  aes_x = "Alcohol",
  aes_y = "Strength",
  title = "Full Model",
  subtitle = paste0("strength ~ alcohol; SSE = ",round(full_sse, digits = 2)),
  rot_y_tic_label = T,
  do_y_title = F
) + geom_line(aes(y = Strength_F), color="red")

layout <- list(
  plots = list(reduced_plot, full_plot),
  rows = c(1,1),
  cols = c(1,2)
)
RplotterPkg::multi_panel_grid(
  layout = layout,
  col_widths = c(6,6),
  row_heights = 6
  #title = "Reduced and Full Model of Alcohol vs Strength"
)

```


4. Estimate F from the full and reduced models:
From above we have from the full *full_ols* and reduced model *reduced_ols*:

Error sum squares:
```{r}
SSE_ur <- full_ols$sse
SSE_r <- reduced_ols$sse
q <- full_ols$k - reduced_ols$k
df_ur <- full_ols$n - full_ols$k - 1

F_val <- ((SSE_r - SSE_ur)/q)/(SSE_ur/df_ur)
```
The $F$ value is `r F_val`


## 6.3 Sequential (or extra) sums of squares

It can be viewed in either of two ways:

a. It is the reduction in the **error sum of the squares** (SSE) when one or more predictor variables are added to the model

b. It is the increase in the **regression sum of the squares** (SSR) when one or more predictors are added to the model

Doing the exercises in this section using the IQ size data.
Setting up the data:
```{r}
data_path <- file.path(current_dir, "data/iqsize.txt")
iqsize_dt <- data.table::fread(data_path)
```

1. $X_{Brain}$ as the only predictor:
```{r}
brain_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain
)
RplotterPkg::create_table(x = brain_ols$anova_df)
```
SSE(Brain) = 16197
SSR(Brain) = 2697
SSTO(Brain) = 18895

2. Fit with regressors $X_{Brain}$ and $X_{Height}$
```{r}
brain_height_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain + Height
)
RplotterPkg::create_table(x = brain_height_ols$anova_df)
```
SSE(Brain,Height) = 13322
SSR(Brain,Height) = 5573
SSTO(Brian,Height) = 18895 -- the same as above

3. Sequential sum of the squares:
Of adding Height where Brain is the only predictor SSR(Height|Brain)

SSR(Height|Brain) = Reduction in error sum of the squares = SSE(Brain) - SSE(Brain,Height) =  16197 - 13322 = 2875

Or

SSR(Height|Brain) = Increase in regression sum of the squares = SSR(Brain,Height) - SSR(Brain) = 5573 - 2697 = 2876

4. (MiniTab question)

5. Order matters -- add the predictors in the reverse order -- $X_{Height}$ then $X_{Brain}$
$X_{Height}$ as the only predictor:
```{r}
height_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Height
)
RplotterPkg::create_table(x = height_ols$anova_df)
```
SSE = 18731
SSR = 164
SSTO = 18895

Now add in the Brain to the model:
```{r}
height_brain_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Height + Brain
)
RplotterPkg::create_table(x = height_brain_ols$anova_df)
```

The sequential sums of the squares:
SSR(Brain|Height) = reduction in sum of squares of errors = SSE(Height) - SSE(Brain|Height) = 18731 - 13322 = 5409

Or
 
SSR(Brain|Height) = increase in sum of squares of regression = SSR(Brain|Height) - SSR(Height) = 5573 - 164 = 5409

6. Sequential sums of the squares for any number of predictors. Build a model in order: $X_{Brain}$, $X_{Height}$, $X_{Weight}$ 

Start with only $X_{Brain}$:
```{r}
brain_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain
)
RplotterPkg::create_table(x = brain_ols$anova_df)
```

SSR(Brain) = 2697
SSE(Brain) = 16197

Add in $X_{Height}$:
```{r}
brain_height_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain + Height
)
RplotterPkg::create_table(x = brain_height_ols$anova_df)
```
 
Sequential increase in regression sum of squares: 
SSR(Height|Brain) = 5573 - 2697 = 2876

Now add in $X_{Weight}$ to the model:
```{r}
brain_height_weight_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain + Height + Weight
)
RplotterPkg::create_table(x = brain_height_weight_ols$anova_df)
```
Sequential increase in regression sum of squares:
SSR(Weight|Brain,Height) = 5572.744 - 5572.741 = 0.003

7. The above was **one-degree-of-freedom** sequential sums of squares -- adding predictors one at a time -- used for testing $H_{0}: \beta_{1} = 0$
**two-degree-of-freedom** sequential sums of squares -- adding two predictors at a time:
Start with only $X_{Brain}$:
```{r}
brain_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain
)
RplotterPkg::create_table(x = brain_ols$anova_df)
```

Now add in $X_{Height}$ and $X_{Weight}$:
```{r}
brain_height_weight_ols <- RregressPkg::ols_calc(
  df = iqsize_dt,
  formula_obj = PIQ ~ Brain + Height + Weight
)
RplotterPkg::create_table(x = brain_height_weight_ols$anova_df)
```

Sequential sum squares:
SSR(Height,Weight|Brain) = increase in regression sum of the squares = 5573 - 2697 = 2876

## 6.4 Hypothesis tests for the slopes
Returning to the 3 hypothesis in section 6.1 above, we will test each hypothesis using *coolhearts.txt* data and the general linear $F$-statistic:
$$ F = \left ( \frac{SSE_{r} - SSE_{ur}}{df_{r} - df_{ur}} \right ) \div \left ( \frac{SSE_{ur}}{df_{ur}} \right ) $$

### 6.4a Set the data

Using the coolhearts data set:
1. Read the data:
```{r}
data_path <- file.path(current_dir, "data/coolhearts.txt")
coolhearts_dt <- data.table::fread(data_path)
coolhearts_dt <- coolhearts_dt[, .(Infect = `Inf`, Area = Area, Early = X2, Late = X3, Group = Group)]
```

### 6.4b Testing all slope parameters equal 0

$H_{0}: \beta_{Area} = \beta_{Early} = \beta_{Late} = 0$

$H_{A}:$ At least one $\beta_{j} \ne 0$ (for $j$ = Area,Early,Late)

Compute the $F$ value:
```{r}
F_lst <- RregressPkg::ols_F_calc(
  df = coolhearts_dt,
  ur_formula_obj = Infect ~ Area + Early + Late,
  r_formula_obj = Infect ~ 1
)
```

The $F$ value is `r F_lst$F_val` with a critical value at 99% probability of `r F_lst$crit`. Thus there is a least one significant predictor.

### 6.4c Testing one slope parameter is 0

$H_{0}: \beta_{Area} = 0$

$H_{A}: \beta_{Area} \ne 0$

Compute the $F$ value:
```{r}
F_lst <- RregressPkg::ols_F_calc(
  df = coolhearts_dt,
  ur_formula_obj = Infect ~ Area + Early + Late,
  r_formula_obj = Infect ~ Early + Late
)
```
The $F$ value is `r F_lst$F_val` with a critical value at 99% probability of `r F_lst$crit`. Thus *Area* is a significant predictor.

### Testing a subset of slope parameters is 0
$H_{0}: \beta_{Early} = \beta_{Late} = 0$

$H_{A}:$ At least one $\beta_{j} \ne 0$ (for $j$ = Early, Late)

Compute the $F$ value:
```{r}
F_lst <- RregressPkg::ols_F_calc(
  df = coolhearts_dt,
  ur_formula_obj = Infect ~ Area + Early + Late,
  r_formula_obj = Infect ~ Area
)
```
The $F$ value is `r F_lst$F_val` with a critical value at 99% probability of `r F_lst$crit`. Thus there is a least one significant predictor in the subset. Or we can say that cooling has a significant influence on the extent of damage that occurs after taking into account the size of the region (*Area*).
